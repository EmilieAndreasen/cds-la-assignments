{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1641/860135159.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "Loading the language model and defining functions. <br>  \n",
    "Specifically, includes two functions: first function cleans the text for information that occurs between '<>', and the second function processes the text according to the assignment requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    return re.sub(r'<.*?>', '', text) #handling 'text' to remove occurances beginning with '<' and ending in '>' (and everything in between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following logic from cds-viz class of making a function that processes a single file at a time, \n",
    "#instead of loading everything in at once\n",
    "def process_text(file_path):\n",
    "    #opening and reading files using 'latin1' as instructed in class\n",
    "    with open(file_path, 'r', encoding='latin1') as f:\n",
    "        text = cleaning_text(f.read())\n",
    "    \n",
    "    #creating a doc object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    #dictionaries to keep track/counting\n",
    "    pos_counts = {'NOUN': 0, 'VERB': 0, 'ADJ': 0, 'ADV': 0}\n",
    "    unique_entities = {'PERSON': set(), 'GPE': set(), 'ORG': set()}\n",
    "    #using for loops to count tokens and entities\n",
    "    for token in doc:\n",
    "        if token.pos_ in pos_counts:\n",
    "            pos_counts[token.pos_] += 1 #if yes, increments counter\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in unique_entities:\n",
    "            unique_entities[ent.label_].add(ent.text) #if yes, adds ent's text \n",
    "\n",
    "    #calculating total numb of tokens/words (only alphabetic characters counted)\n",
    "    total_words = len([token for token in doc if token.is_alpha])\n",
    "    #calculating relative frequencies - utalising dic comprehension\n",
    "    relative_freq = {pos: ((count / total_words) * 10000) for pos, count in pos_counts.items()}\n",
    "\n",
    "    #arranging data for saving\n",
    "    data = {\n",
    "        'RelFreq NOUN': relative_freq.get('NOUN', 0), #specifying 0, to return numb 0 if there are no occurances instead of 'None'\n",
    "        'RelFreq VERB': relative_freq.get('VERB', 0),\n",
    "        'RelFreq ADJ': relative_freq.get('ADJ', 0),\n",
    "        'RelFreq ADV': relative_freq.get('ADV', 0),\n",
    "        'Unique PER': len(unique_entities['PERSON']),\n",
    "        'Unique LOC': len(unique_entities['GPE']),\n",
    "        'Unique ORG': len(unique_entities['ORG']),\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the script\n",
    "Defining the different data paths. <br>  \n",
    "Using a for loop with the functions from above to extract the needed information and output csv files for each subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/EmilieMunchAndreasen#4014/cds-la-assignments/Assignment 1/src'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the data_path and output_path\n",
    "data_path = \"/work/EmilieMunchAndreasen#4014/cds-la-assignments/Assignment 1/data/USEcorpus\"\n",
    "output_path = \"/work/EmilieMunchAndreasen#4014/cds-la-assignments/Assignment 1/out\"\n",
    "dirs = sorted(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loop for iterating over all files in every subfolder\n",
    "for directory in dirs: \n",
    "    subfolder = os.path.join(data_path, directory) #constructing paths\n",
    "    filenames = sorted(os.listdir(subfolder))\n",
    "    \n",
    "    #list to store results in\n",
    "    results = [] \n",
    "    #loops over each file\n",
    "    for text_file in filenames:\n",
    "        file_path = os.path.join(subfolder, text_file)\n",
    "        file_data = process_text(file_path) \n",
    "        results.append({'Filename': text_file, **file_data}) #appends results with filename\n",
    "    #creating pandas df \n",
    "    df = pd.DataFrame(results) \n",
    "    df.to_csv(os.path.join(output_path, f\"{directory}_linguistic_features.csv\"), index=False) #without row indicies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
